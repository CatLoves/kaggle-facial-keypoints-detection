{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Third model: data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "275cadde-3cfc-4dfc-9f84-db7d4bfcb004"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alex/anaconda/lib/python3.5/site-packages/matplotlib/__init__.py:1357: UserWarning:  This call to matplotlib.use() has no effect\n",
      "because the backend has already been chosen;\n",
      "matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "  warnings.warn(_use_error_msg)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg', warn = False)\n",
    "from matplotlib import pyplot\n",
    "from datetime import datetime\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from pandas import DataFrame\n",
    "from pandas.io.parsers import read_csv\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from nolearn.lasagne import BatchIterator\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paths to datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "5af4290b-af7e-4e1e-854e-04e9a6f9d929"
    }
   },
   "outputs": [],
   "source": [
    "FTRAIN = '~/ML/kaggle/facial-keypoints-detection/data/training.csv'\n",
    "FTEST = '~/ML/kaggle/facial-keypoints-detection/data/test.csv'\n",
    "FLOOKUP = '~/ML/kaggle/facial-keypoints-detection/data/IdLookupTable.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "87483b94-afb3-4b70-93d9-6a7de04c6969"
    }
   },
   "outputs": [],
   "source": [
    "def load(test = False, cols = None):\n",
    "#     Loads data from FTEST if *test* is True, otherwise from FTRAIN.\n",
    "#     Pass a list of *cols* if you're only interested in a subset of the\n",
    "#     target columns.\n",
    "    \n",
    "    fname = FTEST if test else FTRAIN\n",
    "    df = read_csv(os.path.expanduser(fname))  # load pandas dataframe\n",
    "\n",
    "    # The Image column has pixel values separated by space; convert\n",
    "    # the values to numpy arrays:\n",
    "    df['Image'] = df['Image'].apply(lambda im: np.fromstring(im, sep = ' '))\n",
    "\n",
    "    if cols:  # get a subset of columns\n",
    "        df = df[list(cols) + ['Image']]\n",
    "\n",
    "    print(df.count())  # prints the number of values for each column\n",
    "    df = df.dropna()  # drop all rows that have missing values in them\n",
    "\n",
    "    X = np.vstack(df['Image'].values) / 255.  # scale pixel values to [0, 1]\n",
    "    X = X.astype(np.float32)\n",
    "\n",
    "    if not test:  # only FTRAIN has any target columns\n",
    "        y = df[df.columns[:-1]].values\n",
    "        y = (y - 48) / 48  # scale target coordinates to [-1, 1]\n",
    "        X, y = shuffle(X, y, random_state=42)  # shuffle train data\n",
    "        y = y.astype(np.float32)\n",
    "    else:\n",
    "        y = None\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "0452913e-e7f0-4a64-becd-f40b6e3be5ae"
    }
   },
   "outputs": [],
   "source": [
    "num_channels = 1 # grayscale\n",
    "image_size = 96\n",
    "\n",
    "def load2d(test = False, cols = None):\n",
    "    X, y = load(test = test)\n",
    "    X = X.reshape(-1, image_size, image_size, num_channels)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "8ff47238-e810-40c4-b2ff-285df7e8317c"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "left_eye_center_x            7039\n",
      "left_eye_center_y            7039\n",
      "right_eye_center_x           7036\n",
      "right_eye_center_y           7036\n",
      "left_eye_inner_corner_x      2271\n",
      "left_eye_inner_corner_y      2271\n",
      "left_eye_outer_corner_x      2267\n",
      "left_eye_outer_corner_y      2267\n",
      "right_eye_inner_corner_x     2268\n",
      "right_eye_inner_corner_y     2268\n",
      "right_eye_outer_corner_x     2268\n",
      "right_eye_outer_corner_y     2268\n",
      "left_eyebrow_inner_end_x     2270\n",
      "left_eyebrow_inner_end_y     2270\n",
      "left_eyebrow_outer_end_x     2225\n",
      "left_eyebrow_outer_end_y     2225\n",
      "right_eyebrow_inner_end_x    2270\n",
      "right_eyebrow_inner_end_y    2270\n",
      "right_eyebrow_outer_end_x    2236\n",
      "right_eyebrow_outer_end_y    2236\n",
      "nose_tip_x                   7049\n",
      "nose_tip_y                   7049\n",
      "mouth_left_corner_x          2269\n",
      "mouth_left_corner_y          2269\n",
      "mouth_right_corner_x         2270\n",
      "mouth_right_corner_y         2270\n",
      "mouth_center_top_lip_x       2275\n",
      "mouth_center_top_lip_y       2275\n",
      "mouth_center_bottom_lip_x    7016\n",
      "mouth_center_bottom_lip_y    7016\n",
      "Image                        7049\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X, y = load2d()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the initial training dataset into training, validation and testing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "50584186-07a8-4328-b7be-880ac0f22dbd"
    }
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
    "x_test, x_valid, y_test, y_valid = train_test_split(x_test, y_test, test_size = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our data augmentation routine. It randomly flips a defined portion of dataset horizontally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FlipBatchIterator(BatchIterator):\n",
    "    flip_indices = [\n",
    "        (0, 2), (1, 3),\n",
    "        (4, 8), (5, 9), (6, 10), (7, 11),\n",
    "        (12, 16), (13, 17), (14, 18), (15, 19),\n",
    "        (22, 24), (23, 25),\n",
    "        ]\n",
    "\n",
    "    def transform(self, Xb, yb):\n",
    "        Xb, yb = super(FlipBatchIterator, self).transform(Xb, yb)\n",
    "\n",
    "        # Flip half of the images in this batch at random:\n",
    "        bs = Xb.shape[0]\n",
    "        indices = np.random.choice(bs, bs / 2, replace=False)\n",
    "        Xb[indices] = Xb[indices, :, :, ::-1]\n",
    "\n",
    "        if yb is not None:\n",
    "            # Horizontal flip of all x coordinates:\n",
    "            yb[indices, ::2] = yb[indices, ::2] * -1\n",
    "\n",
    "            # Swap places, e.g. left_eye_center_x -> right_eye_center_x\n",
    "            for a, b in self.flip_indices:\n",
    "                yb[indices, a], yb[indices, b] = (\n",
    "                    yb[indices, b], yb[indices, a])\n",
    "\n",
    "        return Xb, yb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try flipping 100% images of a sample batch and plot an image to verify it's all good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_sample(x, y, axis):\n",
    "    img = x.reshape(96, 96)\n",
    "    axis.imshow(img, cmap='gray')\n",
    "    axis.scatter(y[0::2] * 48 + 48, y[1::2] * 48 + 48, marker='x', s=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ded1da514de8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbatch_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mbatch_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maugment_flip_random\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# plot two images:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "batch_X = X[0:batch_size, :].copy()\n",
    "batch_y = y[0:batch_size, :].copy()\n",
    "batch_X, batch_y = augment_flip_random(batch_X, batch_y, 1)\n",
    "\n",
    "# plot two images:\n",
    "fig = pyplot.figure(figsize=(6, 3))\n",
    "ax = fig.add_subplot(1, 2, 1, xticks=[], yticks=[])\n",
    "plot_sample(X[1], y[1], ax)\n",
    "ax = fig.add_subplot(1, 2, 2, xticks=[], yticks=[])\n",
    "plot_sample(batch_X[1], batch_y[1], ax)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predefined parameters\n",
    "num_keypoints = 30\n",
    "batch_size = 36\n",
    "num_epochs = 1001\n",
    "\n",
    "model_name = \"3conv_2fc_data_aug_model\"\n",
    "model_variable_scope = model_name\n",
    "model_path = \"/tmp/\" + model_name + \"/model.ckpt\"\n",
    "summaries_dir = \"/tmp/\" + model_name + \"/summaries/\"\n",
    "\n",
    "# We will keep track of runs to visualise training with TensorBoard\n",
    "run_num = 0\n",
    "if tf.gfile.Exists(summaries_dir):\n",
    "    tf.gfile.DeleteRecursively(summaries_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Routines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performs a single fully connected layer pass, e.g. returns `input * weights + bias`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fully_connected(input, size):\n",
    "    weights = tf.get_variable( 'weights', \n",
    "        shape = [input.get_shape()[1], size],\n",
    "        initializer = tf.contrib.layers.xavier_initializer()\n",
    "      )\n",
    "    biases = tf.get_variable( 'biases',\n",
    "        shape = [size],\n",
    "        initializer=tf.constant_initializer(0.0)\n",
    "      )\n",
    "    return tf.matmul(input, weights) + biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Routine for a single convolution layer pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_relu(input, kernel_size, depth):\n",
    "    weights = tf.get_variable( 'weights', \n",
    "        shape = [kernel_size, kernel_size, input.get_shape()[3], depth],\n",
    "        initializer = tf.contrib.layers.xavier_initializer()\n",
    "      )\n",
    "    biases = tf.get_variable( 'biases',\n",
    "        shape = [depth],\n",
    "        initializer=tf.constant_initializer(0.0)\n",
    "      )\n",
    "    conv = tf.nn.conv2d(input, weights,\n",
    "        strides=[1, 1, 1, 1], padding='SAME')\n",
    "    return tf.nn.relu(conv + biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Routine for a pooling layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pool(input, size):\n",
    "    return tf.nn.max_pool(\n",
    "        input, \n",
    "        ksize=[1, size, size, 1], \n",
    "        strides=[1, size, size, 1], \n",
    "        padding='SAME'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Routine that performs entire model pass, e.g. returns model prediction for given input with current model (3 convolution layers with 2 fully connected layers):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_pass(input):\n",
    "    # Convolutions\n",
    "    with tf.variable_scope('conv1'):\n",
    "        conv1 = conv_relu(input, kernel_size = 3, depth = 32) \n",
    "    with tf.variable_scope('pool1'): \n",
    "        pool1 = pool(conv1, size = 2)\n",
    "    with tf.variable_scope('conv2'):\n",
    "        conv2 = conv_relu(pool1, kernel_size = 2, depth = 64)\n",
    "    with tf.variable_scope('pool2'):\n",
    "        pool2 = pool(conv2, size = 2)\n",
    "    with tf.variable_scope('conv3'):\n",
    "        conv3 = conv_relu(pool2, kernel_size = 2, depth = 128)\n",
    "    with tf.variable_scope('pool3'):\n",
    "        pool3 = pool(conv3, size = 2)\n",
    "    \n",
    "    # Fully connected\n",
    "    shape = pool3.get_shape().as_list()\n",
    "    pool3 = tf.reshape(pool3, [-1, shape[1] * shape[2] * shape[3]])\n",
    "    \n",
    "    with tf.variable_scope('fc4'):\n",
    "        fc4 = fully_connected(pool3, size = 500)\n",
    "    with tf.variable_scope('fc5'):\n",
    "        fc5 = fully_connected(fc4, size = 500)\n",
    "    with tf.variable_scope('out'):\n",
    "        prediction = fully_connected(fc5, size = num_keypoints)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculates loss based on model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calc_loss(predictions, labels):\n",
    "    return np.mean(np.square(predictions - labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculates time since `start` and formats as a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_time_hhmmss(start):\n",
    "    end = time.time()\n",
    "    m, s = divmod(end - start, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "    time_str = \"%02d:%02d:%02d\" % (h, m, s)\n",
    "    return time_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed at run time with a training minibatch.\n",
    "    tf_x_batch = tf.placeholder(tf.float32, shape = (None, image_size, image_size, num_channels))\n",
    "    tf_y_batch = tf.placeholder(tf.float32, shape = (None, num_keypoints))\n",
    "\n",
    "    current_epoch = tf.Variable(0)  # count the number of epochs\n",
    "\n",
    "    # Model parameters.\n",
    "    learning_rate = tf.train.exponential_decay(0.03, current_epoch, decay_steps=num_epochs, decay_rate=0.03)\n",
    "    momentum = 0.9 + (0.99 - 0.9) * (current_epoch / num_epochs)\n",
    "    \n",
    "    # Training computation.\n",
    "    with tf.variable_scope(model_variable_scope):\n",
    "        predictions = model_pass(tf_x_batch)\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.square(predictions - tf_y_batch))\n",
    "    #loss = tf.reduce_mean(tf.square(predictions - tf_y_batch)) # + l2_lambda * (tf.nn.l2_loss(weightsInToHid) + tf.nn.l2_loss(weightsHidToOut))  \n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.MomentumOptimizer(\n",
    "        learning_rate = learning_rate, \n",
    "        momentum = momentum, \n",
    "        use_nesterov = True\n",
    "    ).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_predictions_in_batches(X, y, session):\n",
    "    p = []\n",
    "    batch_iterator = BatchIterator(batch_size = 128)\n",
    "    for x_batch, y_batch in batch_iterator(X, y):\n",
    "        [p_batch] = session.run([predictions], feed_dict = {\n",
    "                tf_x_batch : x_batch, \n",
    "                tf_y_batch : y_batch\n",
    "            }\n",
    "        )\n",
    "        p.extend(p_batch)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- EPOCH    0/1001 ---------\n",
      "  Minibatch loss: 0.15726514\n",
      "      Train loss: 0.13676871\n",
      " Validation loss: 0.13722721\n",
      "            Time: 00:00:10\n",
      "--------- EPOCH    1/1001 ---------\n",
      "  Minibatch loss: 0.00569810\n",
      "      Train loss: 0.00679631\n",
      " Validation loss: 0.00748504\n",
      "            Time: 00:00:36\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-f6f86700ebd0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m             feed_dict = {\n\u001b[1;32m     19\u001b[0m                 \u001b[0mtf_x_batch\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m                 \u001b[0mtf_y_batch\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             }\n\u001b[1;32m     22\u001b[0m         )\n",
      "\u001b[0;32m/Users/alex/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    715\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 717\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    718\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/alex/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    913\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 915\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    916\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/alex/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 965\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/alex/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    970\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 972\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    973\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/alex/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m    952\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m    953\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 954\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m    955\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "every_epoch_to_log = 5\n",
    "\n",
    "with tf.Session(graph = graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    saver = tf.train.Saver()\n",
    "    train_loss_history = np.zeros(num_epochs)\n",
    "    valid_loss_history = np.zeros(num_epochs)\n",
    "    print(\"============ TRAINING =============\")\n",
    "    for epoch in range(num_epochs):\n",
    "        # Train on whole randomised dataset in batches\n",
    "        batch_iterator = FlipBatchIterator(batch_size = batch_size, shuffle = True)\n",
    "        for x_batch, y_batch in batch_iterator(x_train, y_train):\n",
    "            session.run([optimizer], feed_dict = {\n",
    "                    tf_x_batch : x_batch, \n",
    "                    tf_y_batch : y_batch\n",
    "                }\n",
    "            )\n",
    "\n",
    "        # If another significant epoch ended, we log our losses.\n",
    "        if (epoch % every_epoch_to_log == 0):\n",
    "            # Get training data predictions and log training loss:\n",
    "            train_loss = calc_loss(\n",
    "                get_predictions_in_batches(x_train, y_train, session), \n",
    "                y_train\n",
    "            )\n",
    "            train_loss_history[epoch] = train_loss\n",
    "\n",
    "            # Get validation data predictions and log validation loss:\n",
    "            valid_loss = calc_loss(\n",
    "                get_predictions_in_batches(x_valid, y_valid, session), \n",
    "                y_valid\n",
    "            )\n",
    "            valid_loss_history[epoch] = valid_loss\n",
    "            \n",
    "            if (epoch % 100 == 0):\n",
    "                print(\"--------- EPOCH %4d/%d ---------\" % (epoch, num_epochs))\n",
    "                print(\"     Train loss: %.8f\" % (train_loss))\n",
    "                print(\"Validation loss: %.8f\" % (valid_loss))\n",
    "                print(\"           Time: \" + get_time_hhmmss(start))\n",
    "\n",
    "    # Evaluate on test dataset.\n",
    "    test_loss = calc_loss(\n",
    "        get_predictions_in_batches(x_test, y_test, session), \n",
    "        y_test\n",
    "    )\n",
    "    print(\"===================================\")\n",
    "    print(\" Test score: %.3f (loss = %.8f)\" % (np.sqrt(test_loss) * 48.0, test_loss)) \n",
    "    print(\" Total time: \" + get_time_hhmmss(start))\n",
    "    \n",
    "    # Save model weights for future use.\n",
    "    save_path = saver.save(session, model_path)\n",
    "    print(\"Model file: \" + save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_axis = np.arange(num_epochs)\n",
    "train_loss_nonzero = train_loss_history > 0\n",
    "valid_loss_nonzero = valid_loss_history > 0\n",
    "\n",
    "pyplot.plot(x_axis[train_loss_nonzero], train_loss_history[train_loss_nonzero], linewidth=3, label=\"train\")\n",
    "pyplot.plot(x_axis[valid_loss_nonzero], valid_loss_history[valid_loss_nonzero], linewidth=3, label=\"valid\")\n",
    "pyplot.grid()\n",
    "pyplot.legend()\n",
    "pyplot.xlabel(\"epoch\")\n",
    "pyplot.ylabel(\"loss\")\n",
    "pyplot.ylim(0.001, 0.01)\n",
    "pyplot.xlim(0, num_epochs)\n",
    "pyplot.yscale(\"log\")\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's take a look at the data and predictions. \n",
    "\n",
    "Load training data, restore saved model parameters and get predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-acfbdf2d9b5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtf_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load' is not defined"
     ]
    }
   ],
   "source": [
    "X, _ = load(test = True)\n",
    "\n",
    "with graph.as_default():\n",
    "    tf_x = tf.constant(X)\n",
    "\n",
    "    with tf.variable_scope(model_variable_scope, reuse = True):\n",
    "        tf_p = model_pass(tf_x)  \n",
    "        \n",
    "with tf.Session(graph = graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    load_path = saver.restore(session, model_path)\n",
    "    p = tf_p.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
